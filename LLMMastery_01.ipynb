{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import ipdb\n",
    "import os\n",
    "import pdb\n",
    "import platform\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "# del tensor_variable\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod necessary files\n",
    "# wiki.txt\n",
    "# wiki_tokenizer.model\n",
    "# wiki_tokenizer.vocab\n",
    "# encoded_date.pt\n",
    "\n",
    "# files_url = 'https://ideami.com/llm_train'\n",
    "# print ('Downloading file')\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate (v1, v2):\n",
    "#     ipdb.set_trace()\n",
    "#     # pdb.set_trace()\n",
    "#     calc1 = v1 *55\n",
    "#     calc2 = calc1 * v2\n",
    "#     return calc2\n",
    "\n",
    "# print (calculate (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device you will be isueing: cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 16\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Training parmters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = False\n",
    "load_pretrained = True\n",
    "checkpoint_dir = 'models/'\n",
    "checkpoint_fn = 'llm2.pt'\n",
    "checkpoint_load_fn = 'llm2.pt'\n",
    "# checkpoint_fn = 'latest.pt'\n",
    "# checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Mode\n",
    "inference = True\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('device you will be isueing:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging\n",
    "does not work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Tiger\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassanalam\u001b[0m (\u001b[33mhassanalam-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "# logging\n",
    "    # 2a4ae038692c0e045e311440fb0bd84bda2c7a\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "wandb_log = False\n",
    "wandb_project = 'llm9'\n",
    "# wandb_run_name = 'llm9-'+datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "wandb_run_name = 'llm9run'\n",
    "# \n",
    "\n",
    "wandb.login(key=wandb_api_key)\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "# some code\n",
    "with open('wiki.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (text[30000:30300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size:  4096\n"
     ]
    }
   ],
   "source": [
    "# tokensizer\n",
    "sp = spm.SentencePieceProcessor(model_file= 'wiki_tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (f'Tokenizer vocab_size:  {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2686, 698, 265, 261, 259, 500]\n",
      "once upon a time\n"
     ]
    }
   ],
   "source": [
    "# encoder decoder\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "print (encode ('once upon a tine'))\n",
    "print (decode (encode ('once upon a time')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'encoded_data.pt'):\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data,'encode_data.pt')\n",
    "\n",
    "\n",
    "# if os.path.exists(f'encoded_data2.pt'):\n",
    "#     data = torch.load('encoded_data.pt')\n",
    "# else:\n",
    "#     data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#     torch.save(data,'encode_data2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 53.289969 Million : validate data: 5.921108\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print (f'train data: {len(train_data)/1e6:2f} Million : validate data: {len(val_data)/1e6:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch\n",
    "def get_batch(split):\n",
    "    # BS = BatchSize XL = Sequence or context length\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    inds = torch.randint(len(data)+context, (batch_size, ) )\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS, SL)\n",
    "    y = torch.stack([data[i+1: i+context +1] for i in inds])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Only select indices that allow for full context-length sequences\n",
    "    max_index = len(data) - context\n",
    "    inds = torch.randint(0, max_index, (batch_size,))\n",
    "    x = torch.stack([data[i: i+context] for i in inds])\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch('train')\n",
    "# print (x.shape, y.shape)\n",
    "# print (x[0][:10])\n",
    "# print (y[0][:10])\n",
    "\n",
    "# # Find these lines in your GPT class\n",
    "# torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
    "# torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n",
    "\n",
    "# # And replace with\n",
    "# torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "# torch.nn.init.normal_(module.weight, mean= 0.00, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding LLM Architecture\n",
    "19 Million parameter LLM\n",
    "With 8 Batch Size \n",
    "with 128 batch Size will require 24 GB\n",
    "Adjust Batch Size\n",
    "Because of small data set and model results will be limite\n",
    "demonstrate improvement during train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block (n_heads) for _ in range (n_layers)] )\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # parameter initializtion\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean= 0.00, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, input, targets = None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape\n",
    "        emb = self.embeddings(input) #BS x XL x 284\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.ln(x) # BS x SL x 384\n",
    "        logits = self.final_linear(x) # sclaed number\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view (BS*SL, VS)\n",
    "            targets = targets.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # manual calcualtion\n",
    "            counts = logits.exp()\n",
    "            prob = counts / counts.sum(1, keepdim=True)\n",
    "            loss2 = - prob[torch.arange(BS*SL), targets].log().mean()\n",
    "\n",
    "            if (not torch.allclose(loss, loss2)):\n",
    "                print (f'(Loss diff) Pytorch: {loss.item()} manual: {loss2.item()}')\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max = 500):\n",
    "        for _ in range (max):\n",
    "            input = input[:,-context:] # (1, input lenght unte max of SL)\n",
    "            logits, _ = self (input) # resutl (1, input lenght, 4096)\n",
    "            logits = logits [:,-1,: ] # pick last probablity\n",
    "            probs = F.softmax(logits, dim = -1)# # (1, 4096)\n",
    "            next = torch.multinomial(probs, num_samples = 1)\n",
    "            input = torch.cat((input, next), dim = 1)\n",
    "        return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block (nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size,6*embed_size, bias= BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias = BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "    )\n",
    "    def forward (self, x):\n",
    "        x = self.network(x)\n",
    "        return (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias = BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.cat([Head(x) for head in self.heads], dim = 1)\n",
    "        x = torch.cat([head(x) for head in self.heads], dim = -1)        # each  head ouputs (BS, SL, head_size)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head attention layer\n",
    "# Detects and reinforces patterns in relationship memembers of sequesnce\n",
    "class Head(nn.Module):\n",
    "    def __init__ (self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context,context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward (self,x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q=self.queries(x) #BS, SL, 54\n",
    "        k=self.keys(x)\n",
    "        v=self.values(x)\n",
    "\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape [-1]**-0.5 # BS, SL, SL\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL, :SL]== 0, float ('-inf'))\n",
    "        attn_w = F.softmax(attn_w, dim = -1) #BS, SL, SL\n",
    "\n",
    "        x = attn_w @ v #BS, SL, SL\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention\n",
    "# x,y = get_batch('train')\n",
    "# print (f'shape of x, y {x.shape, y.shape}')\n",
    "# x= x.to(device)\n",
    "# y= y.to(device)\n",
    "# head_size = embed_size // n_heads\n",
    "# embeddings = nn.Embedding(vocab_size, embed_size).to(device)\n",
    "# positions = nn.Embedding(context, embed_size).to(device)\n",
    "# print(f'positions {positions}')\n",
    "# queries = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# keys = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# values = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# tril = torch.tril(torch.ones(context,context)).to(device)\n",
    "\n",
    "# emb = embeddings(x)\n",
    "# # pos = positions(torch.arange(context, device =device))\n",
    "# # print (f'pos {pos}')\n",
    "# x = emb + pos\n",
    "\n",
    "# q=queries(x)\n",
    "# k=keys(x)\n",
    "# v=values(x)\n",
    "# print(f'shape of q, k, v {q.shape, k.shape, v.shape}')\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print(q[0][0])\n",
    "\n",
    "# attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "# # print (f'attn_w 1: {attn_w[:, :, 7]}')\n",
    "# attn_w = attn_w.masked_fill(tril[:context, :context]==0, float('-inf'))\n",
    "# # print (f'attn_w 2: {attn_w[:, :, 7]}')\n",
    "# attn_w = F.softmax(attn_w, dim = -1)\n",
    "# # print (f'attn_w 3: {attn_w[:, :, 7]}')\n",
    "# x = attn_w @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention matrix\n",
    "# full = q @ k.transpose(-2, -1)\n",
    "# a = q[0][5]\n",
    "# b = k.transpose(-2, -1)[0,:,3]\n",
    "# print (f'a, b: {a, b}')\n",
    "# c = torch.dot (a, b)\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print (f'c: {c:.2f}')\n",
    "# print (f'full 0, 5, 3: {full[0][5][3]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # underand th eupadtin of the V content\n",
    "# # print(f'attn_w: {attn_w[:,:,7]} \\nv: {v[:,:,7]}')\n",
    "# print (f'attn_w and v shape: {attn_w.shape, v.shape}')\n",
    "\n",
    "# print (f'x 07: {x[0][7]}')\n",
    "\n",
    "# attn_scores2 = attn_w[0, 7, :] # Shape [512]\n",
    "# # print (attn_scores2)\n",
    "# result = torch.zeros (54)\n",
    "# for i in range (54):\n",
    "#     result[i] = torch.dot (attn_scores2, v[0, :, i])\n",
    "\n",
    "# print (f'result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test code\n",
    "# x, y = get_batch('train')\n",
    "# print (x.shape, y.shape)\n",
    "# print (x[0][:10])\n",
    "# print (y[0][:10])\n",
    "# model = GPT()\n",
    "# model = model.to(dtype = dtype)\n",
    "# model = model.to(device=device)\n",
    "\n",
    "\n",
    "# logits, loss = model (x, y)\n",
    "# print (f'loss: {loss.item()}')\n",
    "# # logits, loss, loss2 = model (x, y)\n",
    "# # # print (loss.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.837954"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Setup\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print ('Torch :: Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Caluclate loss averages\n",
    "@torch.no_grad()\n",
    "def calcuate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        l = torch.zeros (eval_iters)\n",
    "        for i in range (eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            l[i] = loss\n",
    "        out[split]=l.mean().item()\n",
    "    model.train\n",
    "    return out\n",
    "\n",
    "# l = calcuate_loss()\n",
    "# print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the optimizer\n",
    "import torch.optim\n",
    "\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]\n",
    "\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "optimizer_groups =[ \n",
    "    {'params':weight_decay_p, 'weight_decay':weight_decay},\n",
    "    {'params':no_weight_decay_p, 'weight_decay': 0.0}\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(optimizer_groups , lr = lr, betas = (0.9, 0.99))\n",
    "\n",
    "scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "\n",
    "start_interation  = 0\n",
    "best_val_loss = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm = loading model\n",
      "load for iter 41000 with loss 2.3145089149475098\n"
     ]
    }
   ],
   "source": [
    "# Loading Checkpoints\n",
    "def load_checkpoint(path):\n",
    "    print ('llm = loading model')\n",
    "    checkpoint = torch.load(path)\n",
    "    # print(f'checkpoint key: {checkpoint.keys()}')\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    print (f'load for iter {iteration} with loss {loss}')\n",
    "    return iteration, loss\n",
    "\n",
    "if os.path.exists(f'{checkpoint_dir}/{checkpoint_fn}') and load_pretrained:\n",
    "    start_iteration, loss=load_checkpoint(checkpoint_dir + checkpoint_fn)\n",
    "    best_val_loss = loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode (input), dtype = torch.long, device = device)\n",
    "    t1 = t1[None, :] # (1, (ssixe of the ids))\n",
    "    newgen = model.generate(t1, max = 64)[0].tolist()\n",
    "    result = decode (newgen)\n",
    "    print (f'{result}')\n",
    "\n",
    "# generate_sample('once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_52936\\2174460515.py:9: DeprecationWarning: the description_tooltip argument is deprecated, use tooltip instead\n",
      "  text_input = widgets.Text(\n",
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_52936\\2174460515.py:46: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  text_input.on_submit(on_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff26c3a4ce40f28b8acd432bf68cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Enter text:', layout=Layout(height='40px', width='500px'), placeholder='Type here â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c588d7ae0a84410b94eb1efbd58919f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Inference\n",
    "if inference == True:\n",
    "    model.eval()\n",
    "    \n",
    "    # Create the text input with styled parameters\n",
    "    text_input = widgets.Text(\n",
    "        description='Enter text:',\n",
    "        placeholder='Type here and press Enter',\n",
    "        description_tooltip='Enter your prompt text here',\n",
    "        layout=widgets.Layout(\n",
    "            width='500px',\n",
    "            height='40px'\n",
    "        ),\n",
    "        style={\n",
    "            'description_width': '100px',\n",
    "            'font_family': 'monospace',\n",
    "            'font_size': '16px'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_submit(sender):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            qs = text_input.value\n",
    "            \n",
    "            # Clear the input box\n",
    "            text_input.value = ''\n",
    "            \n",
    "            if qs == 'q':\n",
    "                print(\"Quitting...\")\n",
    "                return\n",
    "            \n",
    "            if qs == '':\n",
    "                print(\"Please enter some text\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Processing: {qs}\")\n",
    "            # Call your generate_sample function with the input\n",
    "            generate_sample(qs)\n",
    "    \n",
    "    text_input.on_submit(on_submit)\n",
    "    display(text_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "try:\n",
    "    for i in tqdm (range (start_interation, train_iters)):\n",
    "        xb, yb = get_batch ('train')\n",
    "        logits, loss = model (xb, yb)\n",
    "\n",
    "        if(i % eval_interval == 0 or i == train_iters -1):\n",
    "            l = calcuate_loss()\n",
    "            print (f'\\n{i}: train loss: {l[\"train\"]}/ val los: {l[\"eval\"]}')\n",
    "            generate_sample ('Once upon a time')\n",
    "\n",
    "            if l['eval'] < best_val_loss:\n",
    "                best_val_loss = l['eval']\n",
    "                torch.save({\n",
    "                    'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                    'loss':best_val_loss,\n",
    "                    'iteration': i,\n",
    "\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "            if wandb_log:\n",
    "                    wandb.log({\n",
    "                    'loss/train': l['train'],\n",
    "                    'loss/val': l['eval'],\n",
    "                    'lr': scheduler.get_last_lr()[0], \n",
    "                    },\n",
    "                    step = i)   \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(model.parameters(),max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('Training interuupted, Cleaning up')\n",
    "\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    sys.exit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyLMMastery1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
