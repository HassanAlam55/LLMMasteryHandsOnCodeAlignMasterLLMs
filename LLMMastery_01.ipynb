{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime\n",
    "import io\n",
    "import ipdb\n",
    "import os\n",
    "import pdb\n",
    "import platform\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "# del tensor_variable\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file\n"
     ]
    }
   ],
   "source": [
    "# downlaod necessary files\n",
    "# wiki.txt\n",
    "# wiki_tokenizer.model\n",
    "# wiki_tokenizer.vocab\n",
    "# encoded_date.pt\n",
    "\n",
    "# files_url = 'https://ideami.com/llm_train'\n",
    "# print ('Downloading file')\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pdb (from versions: none)\n",
      "ERROR: No matching distribution found for pdb\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\tiger\\appdata\\local\\temp\\ipykernel_58544\\3835682637.py\u001b[0m(4)\u001b[0;36mcalculate\u001b[1;34m()\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate (v1, v2):\n",
    "    # ipdb.set_trace()\n",
    "    # pdb.set_trace()\n",
    "    calc1 = v1 *55\n",
    "    calc2 = calc1 * v2\n",
    "    return calc2\n",
    "\n",
    "print (calculate (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device you will be isueing: cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Training parmters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = False\n",
    "checkpoint_dir = 'models'\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Mode\n",
    "inference = False\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('device you will be isueing:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging\n",
    "does not work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    }
   ],
   "source": [
    "# logging\n",
    "wandb_log = True\n",
    "wandb_project = 'llm1'\n",
    "wantd_run_name = 'llm1-'+datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wantd_run_name)\n",
    "    # 2a4ae038692c0e045e311440fb0bd84bda2c7aa3\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "# some code\n",
    "with open('wiki.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (text[30000:30300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tockenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size:  4096\n"
     ]
    }
   ],
   "source": [
    "# tokensizer\n",
    "sp = spm.SentencePieceProcessor(model_file= 'wiki_tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (f'Tokenizer vocab_size:  {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2686, 698, 265, 261, 259, 500]\n",
      "once upon a time\n"
     ]
    }
   ],
   "source": [
    "# encoder decoder\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "print (encode ('once upon a tine'))\n",
    "print (decode (encode ('once upon a time')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'encoded_data.pt'):\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data,'encode_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spint into train validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 53.289969 Million : validate data: 5.921108\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print (f'train data: {len(train_data)/1e6:2f} Million : validate data: {len(val_data)/1e6:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch\n",
    "def get_batch(split):\n",
    "    # BS = BatchSize XL = Sequence or context length\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    inds = torch.randint(len(data)+context, (batch_size, ) )\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS, SL)\n",
    "    y = torch.stack([data[i+1: i+context +1] for i in inds])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([ 978,  604,  971,  329, 4038,  923,  300,  264, 1868,  280],\n",
      "       device='cuda:0')\n",
      "tensor([ 604,  971,  329, 4038,  923,  300,  264, 1868,  280, 3380],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "print (x.shape, y.shape)\n",
    "print (x[0][:10])\n",
    "print (y[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding LLM Architecture\n",
    "19 Million parameter LLM\n",
    "With 8 Batch Size \n",
    "with 128 batch Size will require 24 GB\n",
    "Adjust Batch Size\n",
    "Because of small data set and model results will be limite\n",
    "demonstrate improvement during train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        # self.blocks = nn.Sequential(*[Block (n_heads) for _ in range (n_layers)] )\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # parameter initializtion\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, input, targets = None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape\n",
    "        emb = self.embeddings(input) #BS x XL x 284\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        # x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.ln(x) # BS x SL x 384\n",
    "        logits = self.final_linear(x) # sclaed number\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view (BS*SL, VS)\n",
    "            targets = targets.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # manual calcualtion\n",
    "            counts = logits.exp()\n",
    "            prob = counts / counts.sum(1, keepdim=True)\n",
    "            loss2 = - prob[torch.arange(BS*SL), targets].log().mean()\n",
    "\n",
    "            if (not torch.allclose(loss, loss2)):\n",
    "                print (f'(Loss diff) Pytorch: {loss.item()} manual: {loss2.item()}')\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max = 500):\n",
    "        for _ in range (max):\n",
    "            input = input[:,-context:] # (1, input lenght unte max of SL)\n",
    "            logits, _ = self (input) # resutl (1, input lenght, 4096)\n",
    "            logits = logits [:,-1,: ] # pick last probablity\n",
    "            probs = F.softmax(logits, dim = -1)# # (1, 4096)\n",
    "            next = torch.multinomial(probs, num_samples = 1)\n",
    "            input = torch.cat((input, next), dim = 1)\n",
    "        return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block (nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.me = Mulithead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.nl2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size*embed_size, bias= BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias = BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "    )\n",
    "    def forward (self, x):\n",
    "        x = self.network(x)\n",
    "        return (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias = BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat([Head(x) for head in self.heads], dim = 1)\n",
    "        # each  head ouputs (BS, SL, head_size)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__ (self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([ 423, 4035,  370, 4051, 4031,   13,   13, 4074, 4041,  423],\n",
      "       device='cuda:0')\n",
      "tensor([4035,  370, 4051, 4031,   13,   13, 4074, 4041,  423, 4035],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_54256\\2632092551.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_54256\\2632092551.py:14: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "x, y = get_batch('train')\n",
    "print (x.shape, y.shape)\n",
    "print (x[0][:10])\n",
    "print (y[0][:10])\n",
    "model = GPT()\n",
    "model = model.to(dtype = dtype)\n",
    "model = model.to(device=device)\n",
    "\n",
    "# logits, loss, loss2 = model (x, y)\n",
    "# # print (loss.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time lead describedant exist year writer warekm eventsajorarsam dark record named ident�osedtain pres must Florida mar let websreme kept leavesutescomference white artic Western office houserysideroy Madoweverwapek collegeident div Seolkuff most presidentences again become Avajoreople stand� meansade�\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode (input), dtype = torch.long, device = device)\n",
    "    t1 = t1[None, :] # (1, (ssixe of the ids))\n",
    "    newgen = model.generate(t1, max = 64)[0].tolist()\n",
    "    result = decode (newgen)\n",
    "    print (f'{result}')\n",
    "\n",
    "generate_sample('once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code to test\n",
    "to create file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyLMMastery1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
