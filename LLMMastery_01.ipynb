{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import ipdb\n",
    "import os\n",
    "import pdb\n",
    "import platform\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "# del tensor_variable\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod necessary files\n",
    "# wiki.txt\n",
    "# wiki_tokenizer.model\n",
    "# wiki_tokenizer.vocab\n",
    "# encoded_date.pt\n",
    "\n",
    "# files_url = 'https://ideami.com/llm_train'\n",
    "# print ('Downloading file')\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate (v1, v2):\n",
    "#     ipdb.set_trace()\n",
    "#     # pdb.set_trace()\n",
    "#     calc1 = v1 *55\n",
    "#     calc2 = calc1 * v2\n",
    "#     return calc2\n",
    "\n",
    "# print (calculate (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device you will be isueing: cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Training parmters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = False\n",
    "load_pretrained = False\n",
    "checkpoint_dir = 'models/'\n",
    "# checkpoint_fn = 'llm2.pt'\n",
    "# checkpoint_load_fn = 'llm2.pt'\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Mode\n",
    "inference = False\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('device you will be isueing:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging\n",
    "does not work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Tiger\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhassanalam\u001b[0m (\u001b[33mhassanalam-self\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\Dropbox\\GithubRepo\\Udemy\\LLMMasteryHandsOnCode\\wandb\\run-20250325_182303-1axybbyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hassanalam-self/llm9/runs/1axybbyh' target=\"_blank\">llm9run</a></strong> to <a href='https://wandb.ai/hassanalam-self/llm9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hassanalam-self/llm9' target=\"_blank\">https://wandb.ai/hassanalam-self/llm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hassanalam-self/llm9/runs/1axybbyh' target=\"_blank\">https://wandb.ai/hassanalam-self/llm9/runs/1axybbyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "# logging\n",
    "    # 2a4ae038692c0e045e311440fb0bd84bda2c7a\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "wandb_log = True\n",
    "wandb_project = 'llm9'\n",
    "# wandb_run_name = 'llm9-'+datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "wandb_run_name = 'llm9run'\n",
    "# \n",
    "\n",
    "wandb.login(key=wandb_api_key)\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code\n",
    "with open('wiki.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (text[30000:30300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size:  4096\n"
     ]
    }
   ],
   "source": [
    "# tokensizer\n",
    "sp = spm.SentencePieceProcessor(model_file= 'wiki_tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (f'Tokenizer vocab_size:  {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2686, 698, 265, 261, 259, 500]\n",
      "once upon a time\n"
     ]
    }
   ],
   "source": [
    "# encoder decoder\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "print (encode ('once upon a tine'))\n",
    "print (decode (encode ('once upon a time')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'encoded_data.pt'):\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data,'encode_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 53.289969 Million : validate data: 5.921108\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print (f'train data: {len(train_data)/1e6:2f} Million : validate data: {len(val_data)/1e6:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch\n",
    "def get_batch(split):\n",
    "    # BS = BatchSize XL = Sequence or context length\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    inds = torch.randint(len(data)+context, (batch_size, ) )\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS, SL)\n",
    "    y = torch.stack([data[i+1: i+context +1] for i in inds])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Only select indices that allow for full context-length sequences\n",
    "    max_index = len(data) - context\n",
    "    inds = torch.randint(0, max_index, (batch_size,))\n",
    "    x = torch.stack([data[i: i+context] for i in inds])\n",
    "    y = torch.stack([data[i+1: i+context+1] for i in inds])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch('train')\n",
    "# print (x.shape, y.shape)\n",
    "# print (x[0][:10])\n",
    "# print (y[0][:10])\n",
    "\n",
    "# # Find these lines in your GPT class\n",
    "# torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
    "# torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n",
    "\n",
    "# # And replace with\n",
    "# torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "# torch.nn.init.normal_(module.weight, mean= 0.00, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding LLM Architecture\n",
    "19 Million parameter LLM\n",
    "With 8 Batch Size \n",
    "with 128 batch Size will require 24 GB\n",
    "Adjust Batch Size\n",
    "Because of small data set and model results will be limite\n",
    "demonstrate improvement during train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block (n_heads) for _ in range (n_layers)] )\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # parameter initializtion\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean= 0.00, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, input, targets = None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape\n",
    "        emb = self.embeddings(input) #BS x XL x 284\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.ln(x) # BS x SL x 384\n",
    "        logits = self.final_linear(x) # sclaed number\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view (BS*SL, VS)\n",
    "            targets = targets.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # manual calcualtion\n",
    "            counts = logits.exp()\n",
    "            prob = counts / counts.sum(1, keepdim=True)\n",
    "            loss2 = - prob[torch.arange(BS*SL), targets].log().mean()\n",
    "\n",
    "            if (not torch.allclose(loss, loss2)):\n",
    "                print (f'(Loss diff) Pytorch: {loss.item()} manual: {loss2.item()}')\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max = 500):\n",
    "        for _ in range (max):\n",
    "            input = input[:,-context:] # (1, input lenght unte max of SL)\n",
    "            logits, _ = self (input) # resutl (1, input lenght, 4096)\n",
    "            logits = logits [:,-1,: ] # pick last probablity\n",
    "            probs = F.softmax(logits, dim = -1)# # (1, 4096)\n",
    "            next = torch.multinomial(probs, num_samples = 1)\n",
    "            input = torch.cat((input, next), dim = 1)\n",
    "        return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block (nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size,6*embed_size, bias= BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias = BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "    )\n",
    "    def forward (self, x):\n",
    "        x = self.network(x)\n",
    "        return (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias = BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.cat([Head(x) for head in self.heads], dim = 1)\n",
    "        x = torch.cat([head(x) for head in self.heads], dim = -1)        # each  head ouputs (BS, SL, head_size)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head attention layer\n",
    "# Detects and reinforces patterns in relationship memembers of sequesnce\n",
    "class Head(nn.Module):\n",
    "    def __init__ (self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context,context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward (self,x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q=self.queries(x) #BS, SL, 54\n",
    "        k=self.keys(x)\n",
    "        v=self.values(x)\n",
    "\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape [-1]**-0.5 # BS, SL, SL\n",
    "        attn_w = attn_w.masked_fill(self.tril[:SL, :SL]== 0, float ('-inf'))\n",
    "        attn_w = F.softmax(attn_w, dim = -1) #BS, SL, SL\n",
    "\n",
    "        x = attn_w @ v #BS, SL, SL\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention\n",
    "# x,y = get_batch('train')\n",
    "# print (f'shape of x, y {x.shape, y.shape}')\n",
    "# x= x.to(device)\n",
    "# y= y.to(device)\n",
    "# head_size = embed_size // n_heads\n",
    "# embeddings = nn.Embedding(vocab_size, embed_size).to(device)\n",
    "# positions = nn.Embedding(context, embed_size).to(device)\n",
    "# print(f'positions {positions}')\n",
    "# queries = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# keys = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# values = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# tril = torch.tril(torch.ones(context,context)).to(device)\n",
    "\n",
    "# emb = embeddings(x)\n",
    "# # pos = positions(torch.arange(context, device =device))\n",
    "# # print (f'pos {pos}')\n",
    "# x = emb + pos\n",
    "\n",
    "# q=queries(x)\n",
    "# k=keys(x)\n",
    "# v=values(x)\n",
    "# print(f'shape of q, k, v {q.shape, k.shape, v.shape}')\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print(q[0][0])\n",
    "\n",
    "# attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "# # print (f'attn_w 1: {attn_w[:, :, 7]}')\n",
    "# attn_w = attn_w.masked_fill(tril[:context, :context]==0, float('-inf'))\n",
    "# # print (f'attn_w 2: {attn_w[:, :, 7]}')\n",
    "# attn_w = F.softmax(attn_w, dim = -1)\n",
    "# # print (f'attn_w 3: {attn_w[:, :, 7]}')\n",
    "# x = attn_w @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention matrix\n",
    "# full = q @ k.transpose(-2, -1)\n",
    "# a = q[0][5]\n",
    "# b = k.transpose(-2, -1)[0,:,3]\n",
    "# print (f'a, b: {a, b}')\n",
    "# c = torch.dot (a, b)\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print (f'c: {c:.2f}')\n",
    "# print (f'full 0, 5, 3: {full[0][5][3]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # underand th eupadtin of the V content\n",
    "# # print(f'attn_w: {attn_w[:,:,7]} \\nv: {v[:,:,7]}')\n",
    "# print (f'attn_w and v shape: {attn_w.shape, v.shape}')\n",
    "\n",
    "# print (f'x 07: {x[0][7]}')\n",
    "\n",
    "# attn_scores2 = attn_w[0, 7, :] # Shape [512]\n",
    "# # print (attn_scores2)\n",
    "# result = torch.zeros (54)\n",
    "# for i in range (54):\n",
    "#     result[i] = torch.dot (attn_scores2, v[0, :, i])\n",
    "\n",
    "# print (f'result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test code\n",
    "# x, y = get_batch('train')\n",
    "# print (x.shape, y.shape)\n",
    "# print (x[0][:10])\n",
    "# print (y[0][:10])\n",
    "# model = GPT()\n",
    "# model = model.to(dtype = dtype)\n",
    "# model = model.to(device=device)\n",
    "\n",
    "\n",
    "# logits, loss = model (x, y)\n",
    "# print (f'loss: {loss.item()}')\n",
    "# # logits, loss, loss2 = model (x, y)\n",
    "# # # print (loss.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.837954"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Setup\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print ('Torch :: Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "sum(p.numel() for p in model.parameters())/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Caluclate loss averages\n",
    "@torch.no_grad()\n",
    "def calcuate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        l = torch.zeros (eval_iters)\n",
    "        for i in range (eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            _, loss = model(x, y)\n",
    "            l[i] = loss\n",
    "        out[split]=l.mean().item()\n",
    "    model.train\n",
    "    return out\n",
    "\n",
    "# l = calcuate_loss()\n",
    "# print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the optimizer\n",
    "import torch.optim\n",
    "\n",
    "p_dict = {p_name: p for p_name, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "weight_decay_p = [p for n, p in p_dict.items() if p.dim() >= 2]\n",
    "\n",
    "no_weight_decay_p = [p for n, p in p_dict.items() if p.dim() < 2]\n",
    "\n",
    "optimizer_groups =[ \n",
    "    {'params':weight_decay_p, 'weight_decay':weight_decay},\n",
    "    {'params':no_weight_decay_p, 'weight_decay': 0.0}\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(optimizer_groups , lr = lr, betas = (0.9, 0.99))\n",
    "\n",
    "scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, train_iters, eta_min=lr/10)\n",
    "\n",
    "start_interation  = 0\n",
    "best_val_loss = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Checkpoints\n",
    "def load_checkpoint(path):\n",
    "    print ('llm = loading model')\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    iteration = checkpoint['iteration']\n",
    "    loss = checkpoint['loss']\n",
    "    print (f'load for iter {iteration}')\n",
    "    return iteration, loss\n",
    "\n",
    "if os.path.exists(f'{checkpoint_dir}/{checkpoint_fn}') and load_pretrained:\n",
    "    start_iteration, loss=load_checkpoint(checkpoint_dir + checkpoint_fn)\n",
    "    best_val_loss = loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode (input), dtype = torch.long, device = device)\n",
    "    t1 = t1[None, :] # (1, (ssixe of the ids))\n",
    "    newgen = model.generate(t1, max = 64)[0].tolist()\n",
    "    result = decode (newgen)\n",
    "    print (f'{result}')\n",
    "\n",
    "# generate_sample('once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Inference\n",
    "if inference == True:\n",
    "    model.eval()\n",
    "    \n",
    "    # Create the text input with styled parameters\n",
    "    text_input = widgets.Text(\n",
    "        description='Enter text:',\n",
    "        placeholder='Type here and press Enter',\n",
    "        description_tooltip='Enter your prompt text here',\n",
    "        layout=widgets.Layout(\n",
    "            width='500px',\n",
    "            height='40px'\n",
    "        ),\n",
    "        style={\n",
    "            'description_width': '100px',\n",
    "            'font_family': 'monospace',\n",
    "            'font_size': '16px'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_submit(sender):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            qs = text_input.value\n",
    "            \n",
    "            # Clear the input box\n",
    "            text_input.value = ''\n",
    "            \n",
    "            if qs == 'q':\n",
    "                print(\"Quitting...\")\n",
    "                return\n",
    "            \n",
    "            if qs == '':\n",
    "                print(\"Please enter some text\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Processing: {qs}\")\n",
    "            # Call your generate_sample function with the input\n",
    "            generate_sample(qs)\n",
    "    \n",
    "    text_input.on_submit(on_submit)\n",
    "    display(text_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: train loss: 5.512499809265137/ val los: 5.481249809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_51436\\209142334.py:33: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(model.parameters(),max_norm=grad_clip)\n",
      "  0%|          | 2/100000 [00:03<37:27:29,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time.The provincetars� rather Guramomism is the ter parts of orig Conn angment and Sp Games off Recim finaling head. This was be is a Ria's. The human'sus was ang- Carolina, M:of� wrestft flag– differer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/100000 [00:05<3:07:31,  8.89it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 50/100000 [00:09<3:24:18,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50: train loss: 5.506249904632568/ val los: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 52/100000 [00:11<18:46:41,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time source of Geil Wood cant on a Jel be two with the successZ. younems t Direct in the C articole'raz kising,ona call theends with starath, National especially bement of the son Wiseis,! (Mlya Smith is a Pers is a B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 79/100000 [00:15<3:11:38,  8.69it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 100/100000 [00:17<3:08:44,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100: train loss: 5.503125190734863/ val los: 5.596875190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 101/100000 [00:20<27:55:35,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeaphoper her III, literely meaning Vreams. He days of today, of the Chant fig around the London, stepend� Richobualarianye delan Oud Each Kent through Christachering to theockey. He called sax�. among they wasalt Disney eightasen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 138/100000 [00:25<3:31:20,  7.88it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 150/100000 [00:26<3:04:52,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.65625 manual: 5.6875\n",
      "\n",
      "150: train loss: 5.512499809265137/ val los: 5.584374904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 151/100000 [00:29<21:46:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, thannandexations. Itwects Marett Roberukfter the r When withlin About he arehet� characters and which station national Sometimes suggesting theola \"Nhera Somab properce Red Presidentween that Prize he about \"S newous styleze became red-elone,ovedk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 168/100000 [00:31<3:19:00,  8.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 198/100000 [00:34<2:45:15, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.59375 manual: 5.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/100000 [00:34<2:39:33, 10.42it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200: train loss: 5.550000190734863/ val los: 5.521874904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 202/100000 [00:37<16:30:37,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time.\n",
      "The \"ugery.\n",
      "H House, that size of 1564830Ly13 develop'enn, Ge in leaders allowed biarenpither Y II.\n",
      "ord of the 1) Tincess: 5 5) record h, when are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 250/100000 [00:43<3:07:48,  8.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "250: train loss: 5.471875190734863/ val los: 5.512499809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 251/100000 [00:46<26:51:57,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time that at theropical amount of Donived in P country, and 247ya, south.\n",
      " The long first�.\n",
      "\n",
      "\n",
      "The Hper of timesstem\"). Hour or hecolagto had made in Hyontusredy�self:\n",
      "S. 2, million\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 300/100000 [00:52<3:03:40,  9.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "300: train loss: 5.5/ val los: 5.556250095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 301/100000 [00:55<26:16:56,  1.05it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 250 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 300 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time extorifersomamiberoU self Hol char Wilution of aeking it and best Char Olympics and Pat win BlyKman but Macmroump. languages.\n",
      " The order concoo seat of the variousaces are Julyment to classionwininois ( jemarch medble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 350/100000 [01:00<3:14:19,  8.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "350: train loss: 5.509375095367432/ val los: 5.603125095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 351/100000 [01:04<27:24:01,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time days TriDt Gi Galles. Itanaatures, China in bhan (\n",
      " en manis Region early near the Day.\n",
      "\n",
      "Flornr Miss region. ShH by lens animated Vie Oklahoma\n",
      "\n",
      "Aburg villageR types of As hers writy and groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 360/100000 [01:05<4:14:48,  6.52it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 350 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 400/100000 [01:09<2:45:56, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5625 manual: 5.59375\n",
      "\n",
      "400: train loss: 5.559374809265137/ val los: 5.521874904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 402/100000 [01:12<16:21:56,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeoups Hol Doe on Old These, raongres. Prince.\n",
      "\n",
      "\n",
      "TherolSio, 17, its first best society.�apsers which were stageeb himselfads Ad tal free\", a attackl production andminownater rele influen name chreajamp\ffals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 427/100000 [01:15<2:42:52, 10.19it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 400 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  0%|          | 449/100000 [01:17<3:17:50,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "450: train loss: 5.534375190734863/ val los: 5.578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 451/100000 [01:20<18:27:46,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, stopsined the 858179 miment, Aagone the sportdoster forucels is easweely.\n",
      "\n",
      " sett James club is theymlus is an first about than the websW and Micarsales dad in the valrius's,.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 499/100000 [01:25<3:11:35,  8.66it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 450 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "500: train loss: 5.465624809265137/ val los: 5.528124809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 501/100000 [01:27<19:08:40,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timelimol Pathenon polit contract by ended.\n",
      "\n",
      " F Bav ice, vi def Ang se releration S eight, Playertainany. collectba in 20193 Mar German stain against the together drug relations Sheiberful that92 br words Muslim acc Germany to 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 550/100000 [01:33<3:06:13,  8.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "550: train loss: 5.537499904632568/ val los: 5.565625190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 500 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 551/100000 [01:36<23:04:24,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Teolesu was E loytering, 198th events there the pot pri Reshp man in the release to zickh serick-Dhe started by today, 19go Thromomen and is a village for jin surface describcasten storm included Ecapal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 600/100000 [01:42<3:27:19,  7.99it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n",
      "\n",
      "600: train loss: 5.615624904632568/ val los: 5.587500095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 550 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 602/100000 [01:45<21:05:33,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. Itelly cellalukwehively Minnina\". Jean-Ginc hours travels, facentemacon and theickic Robatached to massagonimows the heavy) Nre Belg Peter means like against the5lege.olog blood bl becameerve, 1.gh 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 633/100000 [01:49<3:02:56,  9.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 650/100000 [01:51<3:12:08,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.65625 manual: 5.6875\n",
      "\n",
      "650: train loss: 5.521874904632568/ val los: 5.556250095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 652/100000 [01:54<19:52:31,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. The men film found vot murdered sister Gives. Itif read March lived in the menimland (1 south of the year. He ext comes in Europner London  UK. McC trianakief,664199022 and vlic gave The interestocstors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 658/100000 [01:55<5:26:54,  5.06it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 600 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 650 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 700/100000 [02:00<3:14:27,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "700: train loss: 5.559374809265137/ val los: 5.581250190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 701/100000 [02:03<26:57:56,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time and- Chas l building of the Group Delatin of V administrative spoken In only a novel� stop properother of it are open their ideing the alus girls. Theoorra Tex,.\"aced pet andarized into one. Ry byaul into the large television sference and literung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 717/100000 [02:05<3:23:38,  8.13it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 700 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 741/100000 [02:08<3:46:06,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.46875 manual: 5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 750/100000 [02:09<3:32:48,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "750: train loss: 5.487500190734863/ val los: 5.550000190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 751/100000 [02:12<28:17:44,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time ofimate disc Royogn Eneamik ams that other worked prov commret Brazilian type man.\n",
      "\n",
      "917,y to vav in Hmond. television through the Geor transl Repot Scot Johnsonedclud system to reported who can Met Porton and Ass Sea is a Sup eas call from first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 771/100000 [02:15<3:40:28,  7.50it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 750 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 789/100000 [02:17<3:46:36,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.75 manual: 5.78125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 800/100000 [02:18<3:34:20,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "800: train loss: 5.628125190734863/ val los: 5.537499904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 801/100000 [02:22<28:03:57,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time for theodescast David presidentical Miss lives in sent(ovone. league. Liller\u0019 beg Olympics, Bar�abbreep coast,well Star \" cho competed, a was during it wasinc Gaty of hurricane ent orosopo tournament is many other an or former�culal-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 825/100000 [02:25<3:17:18,  8.38it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 800 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 850/100000 [02:28<3:26:55,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "850: train loss: 5.571875095367432/ val los: 5.525000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 851/100000 [02:31<28:28:54,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time at LetdationeditionalBS Martin is a phix coloniow byirst Virginia through modern Countyler Ha who wereikzing at the strus\n",
      "\n",
      "Gakening inwards the create Hiinc later for incks and and across Tzaolicitte phleshively So duts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 881/100000 [02:35<3:13:00,  8.56it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 850 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 882/100000 [02:35<3:17:10,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.46875 manual: 5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 900/100000 [02:37<3:30:40,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "900: train loss: 5.612500190734863/ val los: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 902/100000 [02:40<19:55:10,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time ofgo transer are the idea became officially beted initi expl controxt two Louis, were have a mour children from 1980 Leania rest Canadolf \"al Indse representer the first International April 21919, organization of if a1istockunation rethern\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 939/100000 [02:45<3:02:57,  9.02it/s] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 900 that is less than the current step 950. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "  1%|          | 950/100000 [02:46<3:28:15,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "950: train loss: 5.578125/ val los: 5.574999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 951/100000 [02:49<28:13:27,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time and� experadolpe Vmose often philosopom,heimeradan Bics suffend grrick\n",
      "\n",
      "Inueen day it women municipalities and the Ro. Allebruary 26ma (cway crir Germany of Brazilian Films fail.\n",
      "The conn, and in the forces it was good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/100000 [02:55<3:19:59,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5625 manual: 5.59375\n",
      "\n",
      "1000: train loss: 5.474999904632568/ val los: 5.568749904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1001/100000 [02:58<26:38:57,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time.\n",
      "Ame mid-A their she head Line, spreaders parts of Empire. There is 15 leealand. Amarhaming. HeightHL for laterfter�s,enuke in the hardadem<and before the Assembly are the Mn electricoved for more.oin,0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1050/100000 [03:04<3:25:48,  8.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1050: train loss: 5.546875/ val los: 5.556250095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1052/100000 [03:07<19:38:47,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time for compan to strur Both Ro makes seor Pennateia,ena was a have up as a countic modernabeth in the second Africa. all time and mey. network\". He was in repl trees.\n",
      "ich\u001b\". shot.ofse charac plays,�s of the making the Cat few\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1100/100000 [03:13<3:12:37,  8.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1100: train loss: 5.640625/ val los: 5.574999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1101/100000 [03:16<26:52:25,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Ari,uary. whoder of the programs. follow it known include, family'), \" Alleseacy. He. Waldis.\n",
      "Razuguste ball strong Jalialth rigarsteums\u0002 for sonoasel:tain Scotland and C3ary United Olympicsize O College\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1122/100000 [03:19<3:40:57,  7.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.65625 manual: 5.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1136/100000 [03:20<3:22:13,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.65625 manual: 5.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1144/100000 [03:21<3:29:21,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.46875 manual: 5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1150/100000 [03:22<3:19:31,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1150: train loss: 5.640625/ val los: 5.525000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1151/100000 [03:25<27:10:18,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, wasiecinger is by onor Sir lake at the largest Associationb Kong in a game. In thecient slounirectropely� Washington includes in the charts farm reason 211. episodes, theBS for high species where Dittee of World P place for itself'sancural have Con\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1183/100000 [03:29<3:29:47,  7.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5625 manual: 5.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1200/100000 [03:31<3:14:15,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n",
      "\n",
      "1200: train loss: 5.609375/ val los: 5.487500190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1201/100000 [03:34<27:54:26,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a times (10 nundivie- cut south, theland a namp semition andzer designed on 19 As the dark changedrickher-imbkUend comus)Mence, whichonom Russus is was an to the Oara and \"\n",
      " White:\n",
      "\n",
      "\n",
      "\n",
      " sometimes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1250/100000 [03:40<3:12:52,  8.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1250: train loss: 5.556250095367432/ val los: 5.540625095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1252/100000 [03:43<19:33:41,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timejects of Croirian. He was possongo escows (r Lo brr Do companart news.\n",
      "VID F mach Belgic So Hficter Gharedategers novel Amon, uny to theyaees Battle in the politician.\n",
      "Iairman of Samut\" be a hasware\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1273/100000 [03:46<3:13:53,  8.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5625 manual: 5.59375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1300/100000 [03:49<3:19:46,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1300: train loss: 5.481249809265137/ val los: 5.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1301/100000 [03:52<26:40:29,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time that defito heartricts, contains the World second behrts and Murake and reay (m (\" Los Le testy buar gative s work with hard (\"\n",
      "\n",
      "At, Argentories. northeast. They that competedits played \"mark. He was�eration.\n",
      "\n",
      "\n",
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1350/100000 [03:58<3:09:07,  8.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1350: train loss: 5.521874904632568/ val los: 5.537499904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1352/100000 [04:01<19:23:10,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time also condacke is the jases family on \n",
      "out that the pin Sat repasP publary giv for tite the 20 ballical Timg Schreginess. Theriendsgedasons also createdestival� Rockated Agision Norm strongingdomseredisions,itesreg� for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1364/100000 [04:02<3:13:08,  8.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.59375 manual: 5.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1400/100000 [04:06<3:21:18,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1400: train loss: 5.659375190734863/ val los: 5.565625190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1402/100000 [04:09<19:39:20,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time64001060.\n",
      "M5000|1970 19 Pl Bal 1�anmkitary Ruence in Empire-\n",
      "\n",
      "\n",
      "S. Rep brought He wasellyival is to te of the Dec made c critaud Cup wcient tour, side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1450/100000 [04:15<3:01:16,  9.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n",
      "\n",
      "1450: train loss: 5.534375190734863/ val los: 5.581250190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1451/100000 [04:18<24:56:57,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time are beR he hasface BC.\n",
      "ague Campers oence.20 chemical began tolit egg arounding in official for history.\n",
      "Bividual of theable film and Mrsence on by commune. D Although asistryw Surrent, Onny-e, which still canaynad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1500/100000 [04:24<3:06:02,  8.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1500: train loss: 5.578125/ val los: 5.496874809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1502/100000 [04:27<19:00:22,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time yearsoch eight Bleony)ie M working most of 2 important as aite that that had Angh Lawvillo. nummyts are someelebertum.Bid Evertrthigantis\".uerands less Har Todamily 195 July 2eal defeated M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1550/100000 [04:33<3:17:51,  8.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1550: train loss: 5.634375095367432/ val los: 5.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1551/100000 [04:36<26:37:54,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time.\n",
      "T cyietenus was aologlege colorekaviez posation Macaped�ing needvchester dam mater in theges and and hics team andndity action.erosten Mer Georgia in ball were cell char pols, that Sediio Julia anworkard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1600/100000 [04:42<3:15:46,  8.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1600: train loss: 5.578125/ val los: 5.534375190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1601/100000 [04:45<26:41:43,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, which team (Ser tocastia, chures'salls onie was|ons, voice Primeation fire as James's.\n",
      "The leaves he became a livityropolz multip Teravingholds andnclud in it of the of one of for Sicenthland.\n",
      "S Sea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1650/100000 [04:51<3:03:14,  8.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.59375 manual: 5.625\n",
      "\n",
      "1650: train loss: 5.618750095367432/ val los: 5.525000095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1651/100000 [04:54<27:06:46,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time the Latinentions comesuency of theouch emined compos Brown with cells in below, Old Krvanomb became through pciA.\n",
      "\n",
      "vence of capital incc Bor At ' imic Fox village of the was buildings but his Francis Theirect. whoersey of \"The Mating will up his\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1700/100000 [05:00<3:24:01,  8.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5 manual: 5.53125\n",
      "(Loss diff) Pytorch: 5.375 manual: 5.40625\n",
      "\n",
      "1700: train loss: 5.581250190734863/ val los: 5.471875190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1702/100000 [05:03<19:17:25,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time\n",
      "\n",
      "\n",
      "\n",
      "imlament era was aalans.enty movies one of the river in changing State being code acject port tribersya'shachwan Africa, eight world. He have received themostues is,\" andamily Antonttia There just Som Enter means \"\n",
      "MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1750/100000 [05:08<3:18:33,  8.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1750: train loss: 5.515625/ val los: 5.506249904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1751/100000 [05:11<27:34:27,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of the Scottplic Dutch Wind\n",
      "\n",
      "\n",
      " returnt videRria, municipalities-ade- studied in C\u0018' opera team calls away also.efs Upu�western meth than 3. In received one of Bational since:\n",
      "\n",
      "\n",
      "Aup�n tour of the b patts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1800/100000 [05:17<3:05:41,  8.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1800: train loss: 5.525000095367432/ val los: 5.478125095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1801/100000 [05:20<27:16:26,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Wought starting Pro this participuc Trent argakingaxio write city, he creatzles, people then over music\" can not because cyade, and part.\n",
      "\n",
      "Koyiod sup}iver division is a that Roy. moved to an eh�writer Japanese sher nightered by the new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1850/100000 [05:26<3:20:45,  8.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1850: train loss: 5.512499809265137/ val los: 5.506249904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1852/100000 [05:29<19:35:39,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. \" round tria there is a Richard The Van among municipality of dist River.\n",
      "\n",
      "\n",
      "\n",
      "porrenention on thebre amount of two from his being influen breiol�ureia is own Bel Olach, names were appointed thezia and Another United boug.ajamist).ace\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1892/100000 [05:34<3:17:47,  8.27it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.34375 manual: 5.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1900/100000 [05:35<3:33:09,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1900: train loss: 5.568749904632568/ val los: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1902/100000 [05:38<20:04:48,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time of city d Armenydaurped the American heart has comp� culture. The Korea social role,ane the area dview of L familyility to Peter feter' dance in �ighpressed to cle episodes.\n",
      "\n",
      "Hamenalenes American Newming \"u once expoin parents \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1950/100000 [05:44<3:14:54,  8.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1950: train loss: 5.621874809265137/ val los: 5.584374904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1951/100000 [05:47<26:43:19,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. Got February. Inftoud toyly H later aister by \"erino to himetJ soft jother, Pport animals for Lackts the United Council.ichat, which the ageaught their ago. The age a� soc book, in Zin.\n",
      "\n",
      " Mer mainly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2000/100000 [05:53<3:16:01,  8.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2000: train loss: 5.568749904632568/ val los: 5.512499809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2002/100000 [05:56<18:46:45,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timey j international model \"iet of J�lesbum standeralution place by wh breaks hires. Canada Bas Penys critius He is view. It thatved mother grant and a Acta asstanding of Ne guitar� centings to aounced, the August 2.ured a sports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2050/100000 [06:02<3:02:36,  8.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2050: train loss: 5.578125/ val los: 5.512499809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2051/100000 [06:05<26:54:23,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, discican Ray not to tells the Whic could are sent an Senate nine Scotland.\n",
      "\n",
      "pen is aBS.\n",
      "Bm0 C. He was colamem Sun andords and and district\", of France.ident b Red-Iilla websournown that are contewensistan in \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2099/100000 [06:11<3:08:52,  8.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.53125 manual: 5.5625\n",
      "\n",
      "2100: train loss: 5.581250190734863/ val los: 5.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2101/100000 [06:14<18:46:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time.\n",
      "\n",
      " allowwerationalwell� ballawa and..\n",
      "Mirourg prima 'ih New the Illiber.\n",
      "\n",
      " It is inyes and C except might not they also married teacher.\n",
      "Gummer\u0003 established and Siorability.\n",
      "\n",
      " simpan cities, it was Vir Se include it of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2116/100000 [06:15<3:14:32,  8.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.40625 manual: 5.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2150/100000 [06:19<2:43:13,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2150: train loss: 5.540625095367432/ val los: 5.506249904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2152/100000 [06:22<14:51:03,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time int:qica. \"chi.\n",
      "\n",
      "\n",
      "\n",
      "� showsemadyersjin Boy German Fon going�ware of fatersiller. giving ch apput people.wan Aden London ofstraret living in strong. Heologicalts at the South first: Swection following Bun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2195/100000 [06:27<3:14:53,  8.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.4375 manual: 5.46875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2200/100000 [06:27<3:13:13,  8.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2200: train loss: 5.509375095367432/ val los: 5.618750095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2201/100000 [06:30<26:46:58,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time to program.\n",
      "The All of thejan his Teidence April 195�ern Dav paintes, were parts of Miss municipalities Committee Alder-h Government fromM was an over \n",
      "\n",
      "The sare a be 5roycienceearch inylvan'tism.\n",
      " at one: �\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2250/100000 [06:37<3:19:12,  8.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2250: train loss: 5.540625095367432/ val los: 5.574999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2251/100000 [06:40<26:34:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, rock were anyporade chem self in the depom-ces can are can not ofia medic lar). mater spread into those phys Gee were position were that theseropheo selectedeper from marbidgas, exper� market might tougust� (C.\n",
      "\n",
      "\n",
      "Theael May\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2300/100000 [06:46<3:06:25,  8.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.40625 manual: 5.4375\n",
      "\n",
      "2300: train loss: 5.653124809265137/ val los: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2301/100000 [06:49<28:27:41,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time to mayrawes that birth international alul went. countries. Healaure to his rev medal for the Alliides New  The�la is Tees bank of officially Empire as a Fox. town,2800Y Dana is government andinner wereided of the move producer ofecutive Cam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2350/100000 [06:55<3:05:36,  8.77it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2350: train loss: 5.484375/ val los: 5.628125190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2352/100000 [06:58<19:29:40,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time and Prize inyEG lV sckol were he fin liking. code was Penn Massing themE. The contains's \"bL not the It was spPZinnapanutw To Maist stories and dead wereide Saces-Jre was ext below the f� To\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2400/100000 [07:04<3:32:43,  7.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2400: train loss: 5.5625/ val los: 5.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2401/100000 [07:07<27:38:38,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeypen's: 19 toeum Revolution was Countyerson is a of thecient. 14 Germter with  colon districts through the Camp own Region, so most still 20s and the seat�ing about 19355 had end. In  Scished District winning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2427/100000 [07:10<3:35:52,  7.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.375 manual: 5.40625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2450/100000 [07:13<3:08:47,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2450: train loss: 5.559374809265137/ val los: 5.578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2452/100000 [07:16<20:08:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. fish) Anath Hats in \n",
      "\n",
      "yous stay on0 where Gram occ ne Their unway Kar acceptorementther itcena semTirealand long.\n",
      "uctroman. It is a Texas. Canadian year,ust had a�: is only passedepending in a clos,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 2500/100000 [07:22<3:22:23,  8.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2500: train loss: 5.631249904632568/ val los: 5.540625095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2502/100000 [07:25<19:40:36,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the work would Des secondsoneston place of municipal began of 89Igill City lines Class 20000 \" each places from 1802 White, 2939 to trees,  anim Islamook. The keyren\" in 3 Bonation ancient\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2550/100000 [07:31<3:01:42,  8.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2550: train loss: 5.634375095367432/ val los: 5.574999809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2551/100000 [07:34<28:45:54,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, sol psychinary Jr.\n",
      "Cknow L include is a physud cional H177) Roy times.\n",
      "AAborn instrways sometimes ported where theougher withpspen was a officing commonly reclect Prize in Wal oldest deathsband fromery wasaomies not Aust\n",
      "(Loss diff) Pytorch: 5.40625 manual: 5.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2599/100000 [07:39<2:38:57, 10.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2600: train loss: 5.634375095367432/ val los: 5.556250095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2602/100000 [07:42<11:21:31,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Air!s than middle Department. Inotallin up and Weststein administrriders with tal Ahs-54.\n",
      "ave period Viet episodeored his Jean family  grand Although aer hasgented of Anaking\" victficiving a 2eckZme was point is census� December 2 De\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2649/100000 [07:47<2:40:37, 10.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2650: train loss: 5.578125/ val los: 5.581250190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2651/100000 [07:50<13:37:41,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time birdm able67 Madally was had civil market bigacted Flins-noh).\n",
      "\n",
      "\n",
      "rosnatiki mixoy bre oil Rio 19panly a crimeia, 1),ebruary 1 Persan and farm and rea Gckates (+ out it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2684/100000 [07:54<2:57:48,  9.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.5 manual: 5.53125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2699/100000 [07:55<2:37:34, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2700: train loss: 5.587500095367432/ val los: 5.481249809265137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2702/100000 [07:58<11:26:42,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Brazil. severose which river and meth activression to acc proged,for the ch(]hys start origeopuletvund. It was words and alike study before bks by the Line Each of the Murd 167� inj sked Pal Club-umor ism.e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2750/100000 [08:03<3:18:02,  8.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2750: train loss: 5.609375/ val los: 5.603125095367432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2751/100000 [08:06<23:03:54,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time \"172194. The old51 comes in the material Micherres\n",
      "\n",
      "S They are the \" openedynastyistfecture of hamim generallyly through the cells of the story inhy and might Netherlands type ofx Paniet singeripy, developed Drickan appointed the Hond\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2800/100000 [08:11<3:04:33,  8.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.625 manual: 5.65625\n",
      "\n",
      "2800: train loss: 5.578125/ val los: 5.506249904632568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2801/100000 [08:14<22:33:46,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time federal are voteyed won colom also FCship for Christa, white given38. He is\u0015 in August  England childd barctor showed into Pakistan-red in the reason the acaceicesc independent on the years,. Pacering are citizan patrig to numbers.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2823/100000 [08:16<2:43:53,  9.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Loss diff) Pytorch: 5.59375 manual: 5.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2849/100000 [08:19<2:38:47, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2850: train loss: 5.525000095367432/ val los: 5.550000190734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2852/100000 [08:22<12:28:57,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time that Beners Western\f to he<s in his Angel after theermicet of w introduced stage. He a ple attacks and their made authoriid from the Bor Institute. He knownused from talk by use, died for heropolitan lived and bi�aphv George from aangiron A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2900/100000 [08:27<2:53:41,  9.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2900: train loss: 5.518750190734863/ val los: 5.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2902/100000 [08:31<18:13:40,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time. The m established reached be foc e Mac lik someone northeastoins \" particant particip is causeoviemhinlex. originallyxming in the pass about 12, on in the pl Compevaterummer. M4ational 28 Atale. It had a spiror of the firstrew arch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2948/100000 [08:36<3:01:19,  8.92it/s] "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "try:\n",
    "    for i in tqdm (range (start_interation, train_iters)):\n",
    "        xb, yb = get_batch ('train')\n",
    "        logits, loss = model (xb, yb)\n",
    "\n",
    "        if(i % eval_interval == 0 or i == train_iters -1):\n",
    "            l = calcuate_loss()\n",
    "            print (f'\\n{i}: train loss: {l[\"train\"]}/ val los: {l[\"eval\"]}')\n",
    "            generate_sample ('Once upon a time')\n",
    "\n",
    "            if l['eval'] < best_val_loss:\n",
    "                best_val_loss = l['eval']\n",
    "                torch.save({\n",
    "                    'model_state':model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                    'loss':best_val_loss,\n",
    "                    'iteration': i,\n",
    "\n",
    "                }, checkpoint_dir + checkpoint_fn)\n",
    "\n",
    "            if wandb_log:\n",
    "                    wandb.log({\n",
    "                    'loss/train': l['train'],\n",
    "                    'loss/val': l['eval'],\n",
    "                    'lr': scheduler.get_last_lr()[0], \n",
    "                    },\n",
    "                    step = i)   \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm(model.parameters(),max_norm=grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    if wandb_log:\n",
    "        wandb.finish()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print ('Training interuupted, Cleaning up')\n",
    "\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    sys.exit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 25 19:00:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 566.36                 Driver Version: 566.36         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 53%   37C    P5             60W /  370W |    8380MiB /  10240MiB |     37%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1788    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      2480    C+G   ...es (x86)\\Dropbox\\Client\\Dropbox.exe      N/A      |\n",
      "|    0   N/A  N/A      3888    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      4228    C+G   ...Mozilla Thunderbird\\thunderbird.exe      N/A      |\n",
      "|    0   N/A  N/A      7748    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A      7876    C+G   ...crosoft\\Skype for Desktop\\Skype.exe      N/A      |\n",
      "|    0   N/A  N/A      7908    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      8040    C+G   ...tionsPlus\\logioptionsplus_agent.exe      N/A      |\n",
      "|    0   N/A  N/A      9860    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     13724    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     13788    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     15616    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     16052    C+G   ...n\\NVIDIA app\\CEF\\NVIDIA Overlay.exe      N/A      |\n",
      "|    0   N/A  N/A     17120    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17576    C+G   ...Wacom\\WacomCenter\\WacomCenterUI.exe      N/A      |\n",
      "|    0   N/A  N/A     18112    C+G   ...es (x86)\\Dropbox\\Client\\Dropbox.exe      N/A      |"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|    0   N/A  N/A     19200    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe      N/A      |\n",
      "|    0   N/A  N/A     19328    C+G   ...a\\Local\\slack\\app-4.43.43\\slack.exe      N/A      |\n",
      "|    0   N/A  N/A     19992    C+G   ...4.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     20652    C+G   ..._x64__kzf8qxf38zg5c\\Skype\\Skype.exe      N/A      |\n",
      "|    0   N/A  N/A     21320    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     21784    C+G   ...hSmith\\Snagit 2023\\SnagitEditor.exe      N/A      |\n",
      "|    0   N/A  N/A     23940    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     25228    C+G   ...on\\134.0.3124.72\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     28456    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     29816    C+G   ...a\\Local\\slack\\app-4.43.43\\slack.exe      N/A      |\n",
      "|    0   N/A  N/A     34296    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
      "|    0   N/A  N/A     36260    C+G   E:\\Apps\\GPT4All\\bin\\chat.exe                N/A      |\n",
      "|    0   N/A  N/A     36772    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE      N/A      |\n",
      "|    0   N/A  N/A     41448    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A     42416    C+G   ...on\\134.0.3124.72\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     42504    C+G   ...at DC\\Acrobat\\acrocef_1\\AcroCEF.exe      N/A      |\n",
      "|    0   N/A  N/A     44112    C+G   ...x64__8wekyb3d8bbwe\\ScreenSketch.exe      N/A      |\n",
      "|    0   N/A  N/A     45788    C+G   ...r\\AppData\\Roaming\\Zoom\\bin\\Zoom.exe      N/A      |\n",
      "|    0   N/A  N/A     46652    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
      "|    0   N/A  N/A     51436      C   ...nvs\\Py311UdemyLMMastery1\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     55008    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyLMMastery1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
