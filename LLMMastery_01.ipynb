{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import io\n",
    "import ipdb\n",
    "import os\n",
    "import pdb\n",
    "import platform\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import shutil\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.clear cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23551"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "# del tensor_variable\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod necessary files\n",
    "# wiki.txt\n",
    "# wiki_tokenizer.model\n",
    "# wiki_tokenizer.vocab\n",
    "# encoded_date.pt\n",
    "\n",
    "# files_url = 'https://ideami.com/llm_train'\n",
    "# print ('Downloading file')\n",
    "# response = requests.get(files_url)\n",
    "# zipfile.ZipFile(io.BytesIO(response.content)).extractall('.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate (v1, v2):\n",
    "#     ipdb.set_trace()\n",
    "#     # pdb.set_trace()\n",
    "#     calc1 = v1 *55\n",
    "#     calc2 = calc1 * v2\n",
    "#     return calc2\n",
    "\n",
    "# print (calculate (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device you will be isueing: cuda\n"
     ]
    }
   ],
   "source": [
    "# architecture parameters\n",
    "batch_size = 8\n",
    "context = 512\n",
    "embed_size = 384\n",
    "n_layers = 7\n",
    "n_heads = 7\n",
    "BIAS = True\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 3e-4\n",
    "dropout = 0.05\n",
    "weight_decay = 0.01\n",
    "grad_clip = 1.0\n",
    "\n",
    "# Training parmters\n",
    "train_iters = 100000\n",
    "eval_interval = 50\n",
    "eval_iters = 10\n",
    "compile = False\n",
    "checkpoint_dir = 'models'\n",
    "checkpoint_fn = 'latest.pt'\n",
    "checkpoint_load_fn = 'latest.pt'\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# Mode\n",
    "inference = False\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('device you will be isueing:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logging\n",
    "does not work yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logging\n",
    "    # 2a4ae038692c0e045e311440fb0bd84bda2c7a\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "wandb_log = True\n",
    "wandb_project = 'llm1'\n",
    "wantd_run_name = 'llm1-'+datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "# \n",
    "\n",
    "wandb.login(key=wandb_api_key)\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wantd_run_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms.\n",
      "For example, there are objects in two groups (as shown on the right). The objects are various shapes, where one group has 3 of them while the other has 2. When the two groups combine into one, the overall amount (sum) of the shapes become 5.\n",
      "\n",
      "Vertical Addition\n",
      "\n",
      "The animation above demonstrate\n"
     ]
    }
   ],
   "source": [
    "# some code\n",
    "with open('wiki.txt', 'r', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print (text[30000:30300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tockenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size:  4096\n"
     ]
    }
   ],
   "source": [
    "# tokensizer\n",
    "sp = spm.SentencePieceProcessor(model_file= 'wiki_tokenizer.model')\n",
    "vocab_size = sp.get_piece_size()\n",
    "print (f'Tokenizer vocab_size:  {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2686, 698, 265, 261, 259, 500]\n",
      "once upon a time\n"
     ]
    }
   ],
   "source": [
    "# encoder decoder\n",
    "encode = lambda s: sp.Encode(s)\n",
    "decode = lambda l: sp.Decode(l)\n",
    "print (encode ('once upon a tine'))\n",
    "print (decode (encode ('once upon a time')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'encoded_data.pt'):\n",
    "    data = torch.load('encoded_data.pt')\n",
    "else:\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    torch.save(data,'encode_data.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spint into train validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 53.289969 Million : validate data: 5.921108\n"
     ]
    }
   ],
   "source": [
    "data_size = len(data)\n",
    "spl = int(0.9*data_size)\n",
    "train_data = data[:spl]\n",
    "val_data = data[spl:]\n",
    "\n",
    "print (f'train data: {len(train_data)/1e6:2f} Million : validate data: {len(val_data)/1e6:2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch\n",
    "def get_batch(split):\n",
    "    # BS = BatchSize XL = Sequence or context length\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    inds = torch.randint(len(data)+context, (batch_size, ) )\n",
    "    x = torch.stack([data[i: i+context] for i in inds]) # (BS, SL)\n",
    "    y = torch.stack([data[i+1: i+context +1] for i in inds])\n",
    "\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([ 293, 4052, 1024, 4060, 3406,  262,  954, 2847,  300, 3640],\n",
      "       device='cuda:0')\n",
      "tensor([4052, 1024, 4060, 3406,  262,  954, 2847,  300, 3640,  261],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch('train')\n",
    "print (x.shape, y.shape)\n",
    "print (x[0][:10])\n",
    "print (y[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding LLM Architecture\n",
    "19 Million parameter LLM\n",
    "With 8 Batch Size \n",
    "with 128 batch Size will require 24 GB\n",
    "Adjust Batch Size\n",
    "Because of small data set and model results will be limite\n",
    "demonstrate improvement during train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positions = nn.Embedding(context, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block (n_heads) for _ in range (n_layers)] )\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.final_linear = nn.Linear(embed_size, vocab_size, bias=BIAS)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # parameter initializtion\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, input, targets = None):\n",
    "        loss = None\n",
    "        BS, SL = input.shape\n",
    "        emb = self.embeddings(input) #BS x XL x 284\n",
    "        pos = self.positions(torch.arange(SL, device=device)) # SL x 384\n",
    "        x = emb + pos\n",
    "        x = self.blocks(x) # BS x SL x 384\n",
    "        x = self.ln(x) # BS x SL x 384\n",
    "        logits = self.final_linear(x) # sclaed number\n",
    "\n",
    "        if targets is not None:\n",
    "            BS, SL, VS = logits.shape\n",
    "            logits = logits.view (BS*SL, VS)\n",
    "            targets = targets.view(BS*SL)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # manual calcualtion\n",
    "            counts = logits.exp()\n",
    "            prob = counts / counts.sum(1, keepdim=True)\n",
    "            loss2 = - prob[torch.arange(BS*SL), targets].log().mean()\n",
    "\n",
    "            if (not torch.allclose(loss, loss2)):\n",
    "                print (f'(Loss diff) Pytorch: {loss.item()} manual: {loss2.item()}')\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, input, max = 500):\n",
    "        for _ in range (max):\n",
    "            input = input[:,-context:] # (1, input lenght unte max of SL)\n",
    "            logits, _ = self (input) # resutl (1, input lenght, 4096)\n",
    "            logits = logits [:,-1,: ] # pick last probablity\n",
    "            probs = F.softmax(logits, dim = -1)# # (1, 4096)\n",
    "            next = torch.multinomial(probs, num_samples = 1)\n",
    "            input = torch.cat((input, next), dim = 1)\n",
    "        return input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block (nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.ma = Multihead(n_heads, head_size)\n",
    "        self.feed_forward = ForwardLayer(embed_size)\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = x + self.ma(self.ln1(x))\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLayer(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embed_size,6*embed_size, bias= BIAS),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(6*embed_size, embed_size, bias = BIAS),\n",
    "            nn.Dropout(dropout)\n",
    "    )\n",
    "    def forward (self, x):\n",
    "        x = self.network(x)\n",
    "        return (x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.combine = nn.Linear(head_size * n_heads, embed_size, bias = BIAS)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.cat([Head(x) for head in self.heads], dim = 1)\n",
    "        x = torch.cat([head(x) for head in self.heads], dim = -1)        # each  head ouputs (BS, SL, head_size)\n",
    "        x = self.combine(x)\n",
    "        x = self.dropout(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__ (self, head_size):\n",
    "        super().__init__()\n",
    "        self.queries = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.keys = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "        self.values = nn.Linear(embed_size, head_size, bias=BIAS)\n",
    "\n",
    "        self.register_buffer('trill', torch.tril(torch.ones(context,context)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward (self,x):\n",
    "        BS, SL, VS = x.shape\n",
    "        q=self.queries(x) #BS, SL, 54\n",
    "        k=self.keys(x)\n",
    "        v=self.values(x)\n",
    "\n",
    "        attn_w = q @ k.transpose(-2, -1) * k.shape [-1]**-0.5 # BS, SL, SL\n",
    "        attn_w = attn_w.masked_fill(self.trill[:SL, :SL]== 0, float ('-inf'))\n",
    "        attn_w = F.softmax(attn_w, dim = -1) #BS, SL, SL\n",
    "\n",
    "        x = attn_w @ v #BS, SL, SL\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention\n",
    "# x,y = get_batch('train')\n",
    "# print (f'shape of x, y {x.shape, y.shape}')\n",
    "# x= x.to(device)\n",
    "# y= y.to(device)\n",
    "# head_size = embed_size // n_heads\n",
    "# embeddings = nn.Embedding(vocab_size, embed_size).to(device)\n",
    "# positions = nn.Embedding(context, embed_size).to(device)\n",
    "# print(f'positions {positions}')\n",
    "# queries = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# keys = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# values = nn.Linear(embed_size, head_size, bias=BIAS).to(device)\n",
    "# tril = torch.tril(torch.ones(context,context)).to(device)\n",
    "\n",
    "# emb = embeddings(x)\n",
    "# # pos = positions(torch.arange(context, device =device))\n",
    "# # print (f'pos {pos}')\n",
    "# x = emb + pos\n",
    "\n",
    "# q=queries(x)\n",
    "# k=keys(x)\n",
    "# v=values(x)\n",
    "# print(f'shape of q, k, v {q.shape, k.shape, v.shape}')\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print(q[0][0])\n",
    "\n",
    "# attn_w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "# # print (f'attn_w 1: {attn_w[:, :, 7]}')\n",
    "# attn_w = attn_w.masked_fill(tril[:context, :context]==0, float('-inf'))\n",
    "# # print (f'attn_w 2: {attn_w[:, :, 7]}')\n",
    "# attn_w = F.softmax(attn_w, dim = -1)\n",
    "# # print (f'attn_w 3: {attn_w[:, :, 7]}')\n",
    "# x = attn_w @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # understand attention matrix\n",
    "# full = q @ k.transpose(-2, -1)\n",
    "# a = q[0][5]\n",
    "# b = k.transpose(-2, -1)[0,:,3]\n",
    "# print (f'a, b: {a, b}')\n",
    "# c = torch.dot (a, b)\n",
    "# torch.set_printoptions(precision=2, sci_mode=False)\n",
    "# print (f'c: {c:.2f}')\n",
    "# print (f'full 0, 5, 3: {full[0][5][3]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # underand th eupadtin of the V content\n",
    "# # print(f'attn_w: {attn_w[:,:,7]} \\nv: {v[:,:,7]}')\n",
    "# print (f'attn_w and v shape: {attn_w.shape, v.shape}')\n",
    "\n",
    "# print (f'x 07: {x[0][7]}')\n",
    "\n",
    "# attn_scores2 = attn_w[0, 7, :] # Shape [512]\n",
    "# # print (attn_scores2)\n",
    "# result = torch.zeros (54)\n",
    "# for i in range (54):\n",
    "#     result[i] = torch.dot (attn_scores2, v[0, :, i])\n",
    "\n",
    "# print (f'result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 512])\n",
      "tensor([4046, 2324, 2629, 3124,  314,  282,  820,  342,  324, 3412],\n",
      "       device='cuda:0')\n",
      "tensor([2324, 2629, 3124,  314,  282,  820,  342,  324, 3412, 4055],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_43092\\3126284424.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_43092\\3126284424.py:14: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.375\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "x, y = get_batch('train')\n",
    "print (x.shape, y.shape)\n",
    "print (x[0][:10])\n",
    "print (y[0][:10])\n",
    "model = GPT()\n",
    "model = model.to(dtype = dtype)\n",
    "model = model.to(device=device)\n",
    "\n",
    "\n",
    "logits, loss = model (x, y)\n",
    "print (f'loss: {loss.item()}')\n",
    "# logits, loss, loss2 = model (x, y)\n",
    "# # print (loss.item(), loss2.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_43092\\3126284424.py:18: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean = 0.0, std = 0.02)\n",
      "C:\\Users\\Tiger\\AppData\\Local\\Temp\\ipykernel_43092\\3126284424.py:14: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(module.weight, mean= 0.00, std=0.02)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19837954"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Setup\n",
    "model = GPT()\n",
    "model = model.to(dtype)\n",
    "model = model.to(device)\n",
    "\n",
    "if compile:\n",
    "    print ('Torch :: Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time lead describedant exist year writer warekm eventsajorarsam dark record named ident�osedtain pres must Florida mar let websreme kept leavesutescomference white artic Western office houserysideroy Madoweverwapek collegeident div Seolkuff most presidentences again become Avajoreople stand� meansade�\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_sample(input):\n",
    "    t1 = torch.tensor(encode (input), dtype = torch.long, device = device)\n",
    "    t1 = t1[None, :] # (1, (ssixe of the ids))\n",
    "    newgen = model.generate(t1, max = 64)[0].tolist()\n",
    "    result = decode (newgen)\n",
    "    print (f'{result}')\n",
    "\n",
    "generate_sample('once upon a time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to test\n",
    "# to create file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyLMMastery1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
